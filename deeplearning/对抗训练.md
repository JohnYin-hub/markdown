## 对抗训练

### 深度学习中的对抗：

- 生成对抗网络（Generative Adversarial Network）
- 对抗攻击、对抗样本领域

### 现实案例：

微软通过RoBERTa+对抗训练在GLUE上超过了原生RoBerta。

### Min-Max

$$
\underset {\theta}{\operatorname{min}}E_{(x,y)\sim D}[\underset{\Delta\in\Omega}{\operatorname{max}}L(x+\Delta x,y;\theta)]
$$

​		$$D$$：训练集	$$\theta$$ : 模型参数	$$L(x,y;\theta)$$ : 单个样本loss

​		$$\Delta x$$ : 对抗扰动（常规约束：$$||\Delta x||\leq \epsilon$$ $$]\epsilon$$ 是一个常数）	$$\Omega$$ : 是扰动空间

**具体目的：**往训练集注入扰动$$\Delta x$$ 构造对抗样本，使用对抗样本去最小化loss来更新参数$$\theta$$

**重点：**

#### 

### 快速梯度法（Fast Gradient Method）

1. 取$$\Delta x$$的方法：
   $$
   \Delta x = \epsilon \nabla _x L(x,y;\theta)
   $$

2. 标准化$$\Delta x$$:
   $$
   \Delta x = \epsilon \frac{\nabla _x L(x,y;\theta)}{||\nabla _x L(x,y;\theta)||}\quad \quad or \quad\quad \epsilon sign(\nabla _x L(x,y;\theta))
   $$
   即：将输入样本向着损失上升的方向再进一步，得到的对抗样本就能造成更大的损失，提高模型的错误率。

3. 优化：
   $$
   \underset{\theta}{min}E_{(x,y)\sim D}[L(x+\Delta x,y;\theta)]
   $$

### CV 连续性 & NLP 离散性

- CV像素为连续的实数向量，$$x+\Delta x$$仍可以是有意义的图像，且可以视为“扰动”

  - CV 任务对抗训练主要是为了提高模型鲁棒性。
- NLP 输入为离散型one-hot向量，输入空间已经被限定，输入“扰动”也被限定。

  - [Adversarial Training Methods for Semi-Supervised Text Classification](https://arxiv.org/abs/1605.07725) 做法是在Embedding 层添加扰动。
  - 仔细思考 Embedding 层的扰动不一定等效于真实输入扰动，但是在实验中显示，在许多任务上，Embedding层的扰动能有效提高模型性能。 
  - 因此，**在 NLP 任务中，对抗训练的角色不再是为了防御基于梯度的恶意攻击，反而更多的是作为一种 regularization，提高模型的泛化能力。** 



