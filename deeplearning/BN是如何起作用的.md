### Batch Normalization 

- 加速训练
- 抗过拟合作用
- 允许使用更大学习率

### BN是如何起作用的

- 早期解释：

  - 将每一层的输入分布都归一化到$$ N(0,1 )$$，从而减少了所谓的Interal Covariate Shift。
- 解释不足性：
  1. 不管那一层的输入都不可能严格满足正太分布。
  2. 该解释原理无法诠释Instance Normalization、Layer Normalization起作用原因。
- 最近解释：[How Does Batch Normalization Help Optimization?](https://arxiv.org/abs/1805.11604) （2018）
  - 作者认为BN主要作用是使得整个损失函数的landscape变的更为平滑，从而使得我们可以更稳定的训练。
  - 主流激活函数来说，$$[-1,1]$$ 是非线性较强的区间，将输入的单位特征归一化为均值为0，方差为1，充分发挥激活函数的非线性能力，将特征很好的区分开来。（这个是归一化的结论，不是batch归一化的结论）
  - **减去均值那一项，有助于降低神经网络梯度的LL常数，而除以标准差的那一项，更多的是起到类似自适应学习率的作用，使得每个参数的更新更加同步，而不至于对某一层、某个参数过拟合。**（太复杂了证明很麻烦，看结论用心领会）

### Lipschitz continuity（利普希茨连续）

- **wikipedia解释：** **利普希茨连续（Lipschitz continuity）**以德国数学家[鲁道夫·利普希茨](https://zh.wikipedia.org/wiki/鲁道夫·利普希茨)命名，是一个比通常[连续](https://zh.wikipedia.org/wiki/連續函數)更强的光滑性条件。直觉上，利普希茨连续函数限制了函数改变的速度，符合利普希茨条件的函数的斜率，必小于一个称为利普希茨常数的实数（该常数依函数而定）。

- **粗浅理解：** **L 约束** 我们希望模型对输入扰动是不敏感的，这通常也能提高模型泛化能力。我们希望$$||x1-x2||$$ 很小时，$$||fw(x1)-fw(x2)||$$ 也尽可能的小。

- **Lipschitz 公式：**
  $$
  ||f_w(x_1)-f_w(x_2)||\leq C(w) \cdot||x_1-x_2||
  $$
  

​	   这里的$$C$$ 只与模型参数有关，并且希望$$C$$越小越好，越小意味着模型对输入扰动越不敏感，泛化性可能会更好。

- ### Forbenius 范数

  $$
  ||W||_F = \sqrt{\sum_{i,j}w_{i,j}}\tag{1}
  $$

  根据**范数矩阵的相容性**：$$||AB||\leq ||A||\cdot||B||$$	

  于是有：
  $$
  ||W_x||\leq||W||_F \cdot||x||\tag{2}
  $$

- ### L2 正则

​	对于线性模型：
$$
||W(x_1)-W(x_2)|| \leq C(W)\cdot ||x_1-x_2||
$$
​	上述式子在**$$x_1,x_2$$ 非常接近的时候**可以等价于：
$$
||W(x_1-x_2)||\leq C(W)\cdot||x_1-x_2||
$$
​	根据p—范数（欧几里得范数）性质：
$$
||W(x_1-x_2)||\leq {||W||}^2\cdot||x_1-x_2||\tag{3}
$$
​	对比式子(1),(2),(3)就可以发现这里的$$||W||^2$$就是$$\sum_{i,j}w_{ij}^2$$，这就是L2正则项，我们希望该项越小则**L 约束**就越小。
